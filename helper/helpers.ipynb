{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert python txt <p1> <p2> file to json file format\n",
    "import json\n",
    "f = open(\"mapping_db_wk_clean.txt\",'r',encoding='utf-8').read()\n",
    "json_dict = {\"values\" : []}\n",
    "for line in f.split(\"\\n\"):\n",
    "    values = line.split(\" \")\n",
    "    if len(values) == 2 : db,wk = values\n",
    "    json_dict[\"values\"].append({\"db\" : db, \"wk\": wk})\n",
    "with open(\"mapping_db_wk.json\", \"w\") as outfile:\n",
    "    json.dump(json_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#compare sameAs prop with clean db prop\n",
    "with open('mapping_db_wk.json', 'r') as openfile:\n",
    "    mapping_dict = json.load(openfile)\n",
    "\n",
    "#load mapping into a dict to get constant acces time\n",
    "db_sameAs_dict = {}\n",
    "for i in mapping_dict[\"values\"]:\n",
    "    db_sameAs_dict[i[\"db\"]] = i[\"wk\"]\n",
    "\n",
    "with open(\"../data/property_support.json\") as json_file:\n",
    "        property_dictionnary = json.load(json_file)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter prop count dictionary with sameAs dictionary\n",
    "filtered_prop = {}\n",
    "for i in db_sameAs_dict.keys():\n",
    "    val = property_dictionnary.get(i)\n",
    "    if val: filtered_prop[i] = val\n",
    "\n",
    "#sort the dict\n",
    "prop = dict(sorted(filtered_prop.items(), key=lambda item: item[1], reverse=True))\n",
    "#keep only prop with support > 1000\n",
    "prop = dict(filter(lambda x,: True if x[1] > 1000 else False, prop.items()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<http://www.wikidata.org/prop/P570>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_sameAs_dict['<http://dbpedia.org/ontology/deathDate>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http://www.wikidata.org/prop/P11143> sameAs <http://dbpedia.org/ontology/wikiPageID> : 1406259\n",
      "<http://www.wikidata.org/prop/P2451> sameAs <http://dbpedia.org/ontology/birthDate> : 471625\n",
      "<http://www.wikidata.org/prop/P570> sameAs <http://dbpedia.org/ontology/deathDate> : 170365\n",
      "<http://www.wikidata.org/prop/P2907> sameAs <http://dbpedia.org/ontology/utcOffset> : 163232\n",
      "<http://www.wikidata.org/prop/P6499> sameAs <http://dbpedia.org/ontology/populationTotal> : 154981\n",
      "<http://www.wikidata.org/prop/P8497> sameAs <http://dbpedia.org/ontology/Person/height> : 111006\n",
      "<http://www.wikidata.org/prop/P8483> sameAs <http://dbpedia.org/ontology/postalCode> : 106248\n",
      "<http://www.wikidata.org/prop/P5286> sameAs <http://dbpedia.org/ontology/runtime> : 97818\n",
      "<http://www.wikidata.org/prop/P1477> sameAs <http://dbpedia.org/ontology/originalName> : 70857\n",
      "<http://www.wikidata.org/prop/P10708> sameAs <http://dbpedia.org/ontology/areaCode> : 66970\n",
      "<http://www.wikidata.org/prop/P10673> sameAs <http://dbpedia.org/ontology/releaseDate> : 61238\n",
      "<http://www.wikidata.org/prop/P112> sameAs <http://dbpedia.org/ontology/foundingYear> : 50531\n",
      "<http://www.wikidata.org/prop/P11054> sameAs <http://dbpedia.org/ontology/number> : 43027\n",
      "<http://www.wikidata.org/prop/P10268> sameAs <http://dbpedia.org/ontology/imdbId> : 35122\n",
      "<http://www.wikidata.org/prop/P6833> sameAs <http://dbpedia.org/ontology/title> : 34916\n",
      "<http://www.wikidata.org/prop/P2918> sameAs <http://dbpedia.org/ontology/address> : 34716\n",
      "<http://www.wikidata.org/prop/P5974> sameAs <http://dbpedia.org/ontology/synonym> : 23916\n",
      "<http://www.wikidata.org/prop/P6000> sameAs <http://dbpedia.org/ontology/areaWater> : 23761\n",
      "<http://www.wikidata.org/prop/P1546> sameAs <http://dbpedia.org/ontology/motto> : 20162\n",
      "<http://www.wikidata.org/prop/P1113> sameAs <http://dbpedia.org/ontology/numberOfEpisodes> : 20030\n",
      "<http://www.wikidata.org/prop/P10673> sameAs <http://dbpedia.org/ontology/openingDate> : 19088\n",
      "<http://www.wikidata.org/prop/P112> sameAs <http://dbpedia.org/ontology/foundingDate> : 18463\n",
      "<http://www.wikidata.org/prop/P3423> sameAs <http://dbpedia.org/ontology/inseeCode> : 17796\n",
      "<http://www.wikidata.org/prop/P1104> sameAs <http://dbpedia.org/ontology/numberOfPages> : 14315\n",
      "<http://www.wikidata.org/prop/P7708> sameAs <http://dbpedia.org/ontology/isbn> : 13610\n",
      "<http://www.wikidata.org/prop/P2437> sameAs <http://dbpedia.org/ontology/numberOfSeasons> : 12890\n",
      "<http://www.wikidata.org/prop/P2769> sameAs <http://dbpedia.org/ontology/budget> : 12160\n",
      "<http://www.wikidata.org/prop/P6364> sameAs <http://dbpedia.org/ontology/officialSchoolColour> : 12074\n",
      "<http://www.wikidata.org/prop/P3415> sameAs <http://dbpedia.org/ontology/startDate> : 11738\n",
      "<http://www.wikidata.org/prop/P4970> sameAs <http://dbpedia.org/ontology/alternativeName> : 11150\n",
      "<http://www.wikidata.org/prop/P945> sameAs <http://dbpedia.org/ontology/allegiance> : 11028\n",
      "<http://www.wikidata.org/prop/P1671> sameAs <http://dbpedia.org/ontology/routeNumber> : 10895\n",
      "<http://www.wikidata.org/prop/P6510> sameAs <http://dbpedia.org/ontology/topSpeed> : 10703\n",
      "<http://www.wikidata.org/prop/P1836> sameAs <http://dbpedia.org/ontology/draftPick> : 10397\n",
      "<http://www.wikidata.org/prop/P8093> sameAs <http://dbpedia.org/ontology/oclc> : 10183\n",
      "<http://www.wikidata.org/prop/P735> sameAs <http://dbpedia.org/ontology/formerName> : 9883\n",
      "<http://www.wikidata.org/prop/P6639> sameAs <http://dbpedia.org/ontology/capacity> : 9236\n",
      "<http://www.wikidata.org/prop/P3460> sameAs <http://dbpedia.org/ontology/militaryCommand> : 8992\n",
      "<http://www.wikidata.org/prop/P8703> sameAs <http://dbpedia.org/ontology/abbreviation> : 8383\n",
      "<http://www.wikidata.org/prop/P5606> sameAs <http://dbpedia.org/ontology/broadcastStationClass> : 8280\n",
      "<http://www.wikidata.org/prop/P2545> sameAs <http://dbpedia.org/ontology/bowlingSide> : 7437\n",
      "<http://www.wikidata.org/prop/P2144> sameAs <http://dbpedia.org/ontology/frequency> : 7389\n",
      "<http://www.wikidata.org/prop/P10786> sameAs <http://dbpedia.org/ontology/formationDate> : 7230\n",
      "<http://www.wikidata.org/prop/P822> sameAs <http://dbpedia.org/ontology/mascot> : 6944\n",
      "<http://www.wikidata.org/prop/P4803> sameAs <http://dbpedia.org/ontology/agencyStationCode> : 6929\n",
      "<http://www.wikidata.org/prop/P11113> sameAs <http://dbpedia.org/ontology/facilityId> : 6900\n",
      "<http://www.wikidata.org/prop/P10215> sameAs <http://dbpedia.org/ontology/numberBuilt> : 6551\n",
      "<http://www.wikidata.org/prop/P5209> sameAs <http://dbpedia.org/ontology/isoCodeRegion> : 6411\n",
      "<http://www.wikidata.org/prop/P5677> sameAs <http://dbpedia.org/ontology/strength> : 6315\n",
      "<http://www.wikidata.org/prop/P3661> sameAs <http://dbpedia.org/ontology/managerTitle> : 5745\n",
      "<http://www.wikidata.org/prop/P7363> sameAs <http://dbpedia.org/ontology/issn> : 5642\n",
      "<http://www.wikidata.org/prop/P2262> sameAs <http://dbpedia.org/ontology/draft> : 5299\n",
      "<http://www.wikidata.org/prop/P5800> sameAs <http://dbpedia.org/ontology/role> : 5285\n",
      "<http://www.wikidata.org/prop/P3586> sameAs <http://dbpedia.org/ontology/iupacName> : 5268\n",
      "<http://www.wikidata.org/prop/P9860> sameAs <http://dbpedia.org/ontology/icaoLocationIdentifier> : 5127\n",
      "<http://www.wikidata.org/prop/P8887> sameAs <http://dbpedia.org/ontology/percentageOfAreaWater> : 5010\n",
      "<http://www.wikidata.org/prop/P7779> sameAs <http://dbpedia.org/ontology/militaryUnitSize> : 4670\n",
      "<http://www.wikidata.org/prop/P2660> sameAs <http://dbpedia.org/ontology/prominence> : 4539\n",
      "<http://www.wikidata.org/prop/P3337> sameAs <http://dbpedia.org/ontology/time> : 4354\n",
      "<http://www.wikidata.org/prop/P3931> sameAs <http://dbpedia.org/ontology/licensee> : 4156\n",
      "<http://www.wikidata.org/prop/P6852> sameAs <http://dbpedia.org/ontology/casNumber> : 3924\n",
      "<http://www.wikidata.org/prop/P8502> sameAs <http://dbpedia.org/ontology/floorCount> : 3912\n",
      "<http://www.wikidata.org/prop/P1923> sameAs <http://dbpedia.org/ontology/teamName> : 3635\n",
      "<http://www.wikidata.org/prop/P576> sameAs <http://dbpedia.org/ontology/dissolutionYear> : 3600\n",
      "<http://www.wikidata.org/prop/P627> sameAs <http://dbpedia.org/ontology/iucnCategory> : 3562\n",
      "<http://www.wikidata.org/prop/P8136> sameAs <http://dbpedia.org/ontology/lcc> : 3545\n",
      "<http://www.wikidata.org/prop/P6271> sameAs <http://dbpedia.org/ontology/demonym> : 3542\n",
      "<http://www.wikidata.org/prop/P3640> sameAs <http://dbpedia.org/ontology/fdaUniiCode> : 3521\n",
      "<http://www.wikidata.org/prop/P662> sameAs <http://dbpedia.org/ontology/pubchem> : 3484\n",
      "<http://www.wikidata.org/prop/P2197> sameAs <http://dbpedia.org/ontology/productionEndYear> : 3465\n",
      "<http://www.wikidata.org/prop/P2130> sameAs <http://dbpedia.org/ontology/cost> : 3316\n",
      "<http://www.wikidata.org/prop/P4584> sameAs <http://dbpedia.org/ontology/firstAppearance> : 3307\n",
      "<http://www.wikidata.org/prop/P10038> sameAs <http://dbpedia.org/ontology/identificationSymbol> : 3187\n",
      "<http://www.wikidata.org/prop/P7457> sameAs <http://dbpedia.org/ontology/signature> : 3130\n",
      "<http://www.wikidata.org/prop/P2121> sameAs <http://dbpedia.org/ontology/careerPrizeMoney> : 3130\n",
      "<http://www.wikidata.org/prop/P9255> sameAs <http://dbpedia.org/ontology/iso6393Code> : 3082\n",
      "<http://www.wikidata.org/prop/P8361> sameAs <http://dbpedia.org/ontology/dcc> : 2857\n",
      "<http://www.wikidata.org/prop/P7253> sameAs <http://dbpedia.org/ontology/colourName> : 2783\n",
      "<http://www.wikidata.org/prop/P10786> sameAs <http://dbpedia.org/ontology/acquirementDate> : 2747\n",
      "<http://www.wikidata.org/prop/P2295> sameAs <http://dbpedia.org/ontology/netIncome> : 2642\n",
      "<http://www.wikidata.org/prop/P2669> sameAs <http://dbpedia.org/ontology/dissolutionDate> : 2539\n",
      "<http://www.wikidata.org/prop/P3236> sameAs <http://dbpedia.org/ontology/publicationDate> : 2494\n",
      "<http://www.wikidata.org/prop/P7807> sameAs <http://dbpedia.org/ontology/icd10> : 2455\n",
      "<http://www.wikidata.org/prop/P6340> sameAs <http://dbpedia.org/ontology/ecNumber> : 2434\n",
      "<http://www.wikidata.org/prop/P4508> sameAs <http://dbpedia.org/ontology/bicycleInformation> : 2370\n",
      "<http://www.wikidata.org/prop/P11108> sameAs <http://dbpedia.org/ontology/participant> : 2368\n",
      "<http://www.wikidata.org/prop/P2049> sameAs <http://dbpedia.org/ontology/MeanOfTransportation/width> : 2357\n",
      "<http://www.wikidata.org/prop/P2227> sameAs <http://dbpedia.org/ontology/membership> : 2292\n",
      "<http://www.wikidata.org/prop/P9888> sameAs <http://dbpedia.org/ontology/conservationStatus> : 2246\n",
      "<http://www.wikidata.org/prop/P592> sameAs <http://dbpedia.org/ontology/chEMBL> : 2098\n",
      "<http://www.wikidata.org/prop/P2575> sameAs <http://dbpedia.org/ontology/numberOfVolumes> : 1994\n",
      "<http://www.wikidata.org/prop/P8811> sameAs <http://dbpedia.org/ontology/lccn> : 1968\n",
      "<http://www.wikidata.org/prop/P2146> sameAs <http://dbpedia.org/ontology/orbitalPeriod> : 1948\n",
      "<http://www.wikidata.org/prop/P1692> sameAs <http://dbpedia.org/ontology/icd9> : 1942\n",
      "<http://www.wikidata.org/prop/P575> sameAs <http://dbpedia.org/ontology/discovered> : 1934\n",
      "<http://www.wikidata.org/prop/P8627> sameAs <http://dbpedia.org/ontology/closingDate> : 1878\n",
      "<http://www.wikidata.org/prop/P4118> sameAs <http://dbpedia.org/ontology/faaLocationIdentifier> : 1814\n",
      "<http://www.wikidata.org/prop/P1449> sameAs <http://dbpedia.org/ontology/pseudonym> : 1795\n",
      "<http://www.wikidata.org/prop/P1884> sameAs <http://dbpedia.org/ontology/hairColor> : 1672\n",
      "<http://www.wikidata.org/prop/P6259> sameAs <http://dbpedia.org/ontology/epoch> : 1671\n",
      "<http://www.wikidata.org/prop/P1340> sameAs <http://dbpedia.org/ontology/eyeColor> : 1626\n",
      "<http://www.wikidata.org/prop/P9340> sameAs <http://dbpedia.org/ontology/meshId> : 1499\n",
      "<http://www.wikidata.org/prop/P699> sameAs <http://dbpedia.org/ontology/diseasesDB> : 1451\n",
      "<http://www.wikidata.org/prop/P10135> sameAs <http://dbpedia.org/ontology/recordDate> : 1444\n",
      "<http://www.wikidata.org/prop/P10246> sameAs <http://dbpedia.org/ontology/medlinePlus> : 1375\n",
      "<http://www.wikidata.org/prop/P610> sameAs <http://dbpedia.org/ontology/firstAscentYear> : 1368\n",
      "<http://www.wikidata.org/prop/P7124> sameAs <http://dbpedia.org/ontology/firstPublicationDate> : 1365\n",
      "<http://www.wikidata.org/prop/P5348> sameAs <http://dbpedia.org/ontology/diameter> : 1323\n",
      "<http://www.wikidata.org/prop/P10631> sameAs <http://dbpedia.org/ontology/municipalityCode> : 1307\n",
      "<http://www.wikidata.org/prop/P8811> sameAs <http://dbpedia.org/ontology/coden> : 1294\n",
      "<http://www.wikidata.org/prop/P247> sameAs <http://dbpedia.org/ontology/cosparId> : 1292\n",
      "<http://www.wikidata.org/prop/P715> sameAs <http://dbpedia.org/ontology/drugbank> : 1286\n",
      "<http://www.wikidata.org/prop/P2388> sameAs <http://dbpedia.org/ontology/office> : 1219\n",
      "<http://www.wikidata.org/prop/P10788> sameAs <http://dbpedia.org/ontology/serviceNumber> : 1212\n",
      "<http://www.wikidata.org/prop/P3301> sameAs <http://dbpedia.org/ontology/broadcastTranslator> : 1197\n",
      "<http://www.wikidata.org/prop/P5978> sameAs <http://dbpedia.org/ontology/classification> : 1165\n",
      "<http://www.wikidata.org/prop/P2234> sameAs <http://dbpedia.org/ontology/volume> : 1123\n",
      "<http://www.wikidata.org/prop/P9374> sameAs <http://dbpedia.org/ontology/omim> : 1102\n",
      "<http://www.wikidata.org/prop/P6224> sameAs <http://dbpedia.org/ontology/description> : 1083\n",
      "<http://www.wikidata.org/prop/P673> sameAs <http://dbpedia.org/ontology/eMedicineSubject> : 1071\n",
      "<http://www.wikidata.org/prop/P673> sameAs <http://dbpedia.org/ontology/eMedicineTopic> : 1069\n",
      "<http://www.wikidata.org/prop/P6801> sameAs <http://dbpedia.org/ontology/bedCount> : 1044\n",
      "<http://www.wikidata.org/prop/P8058> sameAs <http://dbpedia.org/ontology/symbol> : 1043\n",
      "<http://www.wikidata.org/prop/P683> sameAs <http://dbpedia.org/ontology/chEBI> : 1012\n"
     ]
    }
   ],
   "source": [
    "for i in prop.keys():\n",
    "        print(f\"{db_sameAs_dict[i]} sameAs {i} : {prop[i]}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "import time\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#const\n",
    "DBPEDIA = \"http://dbpedia.org/sparql\"\n",
    "WIKIDATA = \"https://query.wikidata.org/sparql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isbn - wdt:P957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparql_query(query : str, endpoint : str ) -> dict:\n",
    "    sparql = SPARQLWrapper(\n",
    "        endpoint\n",
    "    )\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    sparql.setQuery(query)\n",
    "\n",
    "    try:\n",
    "        ret = sparql.queryAndConvert()\n",
    "        return ret\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise Exception(\"Error in query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_value_sample(prop : str) -> str:\n",
    "    query = \"\"\"\n",
    "                select ?e ?p ?v where {\n",
    "                    values ?p {\"\"\"+prop+\"\"\"}.\n",
    "                    ?e ?p ?v\n",
    "                } limit 100\n",
    "            \"\"\"\n",
    "    query = re.sub(r\"\\n|'\",\"\",query)\n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find prop to query\n",
    "db_prop = \"<http://dbpedia.org/ontology/releaseDate>\"\n",
    "wk_prop = \"<http://www.wikidata.org/prop/statement/P10673>\"\n",
    "result_db = sparql_query(query_value_sample(db_prop),DBPEDIA)\n",
    "result_wk = sparql_query(query_value_sample(wk_prop),WIKIDATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_i = []\n",
    "for i,j in zip(result_db[\"results\"][\"bindings\"],result_wk[\"results\"][\"bindings\"]):\n",
    "    #print(i)\n",
    "    #print(i['v']['value'],j['v']['value'])\n",
    "    date_i.append(i['v']['value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<http://dbpedia.org/ontology/releaseDate>', '<http://www.wikidata.org/prop/P10673>']\n",
      "['<http://dbpedia.org/ontology/releaseDate>', '<http://www.wikidata.org/prop/P10673>']\n"
     ]
    }
   ],
   "source": [
    "properties_pair_file =\"../data/properties_pair.txt\"\n",
    "db_wk_pairs = []\n",
    "with open(properties_pair_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f.readlines():\n",
    "        print(line.split(\" \"))\n",
    "        if len(splited := line.split(\" \")) == 2:\n",
    "            print(splited)\n",
    "            db_wk_pairs.append((splited[0], splited[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "from functools import reduce \n",
    "import ijson\n",
    "import json\n",
    "import os\n",
    "\n",
    "def property_list_cutting(l: list ,slice_size: int ) -> list[list[str]]:\n",
    "    \"\"\"From a list, return slice 2D with n sublist of size \"slice_size\"\n",
    "\n",
    "    Args:\n",
    "        l (_type_): huge 1D list\n",
    "        slice_size (_type_): 2D list composed of n \"slice_size\" list\n",
    "\n",
    "    Returns:\n",
    "        list : slice_size\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    sliced_list = []\n",
    "    if len(l) < slice_size: return l\n",
    "    while index<len(l):\n",
    "        sliced_list.append(l[index:index+slice_size])\n",
    "        index+=slice_size\n",
    "    return sliced_list\n",
    "\n",
    "def query_generate_VALUES(slice: list) -> str:\n",
    "    \"\"\"From a list return a string composed of all element of the list joined\n",
    "\n",
    "    Args:\n",
    "        slice (list): 1D list\n",
    "\n",
    "    Returns:\n",
    "        str : return a string of the type \" slice[0] slice[1] slice[2] ... \"\n",
    "    \"\"\"\n",
    "    func = lambda acc,x : acc+\" \"+str(x)\n",
    "    acc = \"\"\n",
    "    joined_string = reduce(func, slice, acc)\n",
    "    return joined_string\n",
    "\n",
    "#TODO\n",
    "def read_sameAs_file(file: str, var_names : list[str] = [\"DB\",\"WK\"]) -> tuple[set[str],set[str]]:\n",
    "    \"\"\"Generate a list of sameAs from Dbpedia and Wikidata\n",
    "    Args:\n",
    "        file (str): sameAs json file name\n",
    "    \"\"\"\n",
    "    f_read = open(file, 'r', encoding=\"UTF-8\")\n",
    "\n",
    "    first,second = var_names#refactor with var name header from json file\n",
    "\n",
    "    db_entity_list = set()\n",
    "    wk_entity_list = set()\n",
    "\n",
    "    for record in ijson.items(f_read, \"results.bindings.item\"):\n",
    "        db_entity_list.add(f'<{record[first][\"value\"]}>')\n",
    "        wk_entity_list.add(f'<{record[second][\"value\"]}>')\n",
    "\n",
    "    f_read.close()\n",
    "    return db_entity_list,wk_entity_list\n",
    "\n",
    "\n",
    "def read_ttl_file(file: str) -> tuple[list[str],list[str],list[str]]:\n",
    "    \"\"\"Generate a list entity property value from a ttl file\n",
    "    Args:\n",
    "        file (str): sameAs json file name\n",
    "    \"\"\"\n",
    "    f_read = open(file, 'r', encoding=\"UTF-8\").read()\n",
    "\n",
    "    entity = []\n",
    "    prop = []\n",
    "    value = []\n",
    "\n",
    "    for line in f_read.split(\"\\n\"):\n",
    "        _line = line.split(\"\\t\")\n",
    "        if len(_line) == 3:\n",
    "            e,p,v = _line\n",
    "            entity.append(e)\n",
    "            prop.append(p)\n",
    "            value.append(v)\n",
    "        \n",
    "    return entity,prop,value\n",
    "\n",
    "\n",
    "def read_result_file(file : str, output_file : str, prop_var_name : str) -> None:\n",
    "    \"\"\"Read a result file to fill a dictionary \n",
    "\n",
    "    Args:\n",
    "        file (str): result file in json format\n",
    "    \"\"\"\n",
    "    \n",
    "    f_read = open(file, 'r', encoding=\"UTF-8\")\n",
    "    prop = prop_var_name #name of the sparql var\n",
    "    properties_count_file = output_file #dict file name\n",
    "    #if file exist open json file and load \n",
    "    if os.path.exists(properties_count_file):\n",
    "        with open(properties_count_file) as json_file:\n",
    "            property_dictionnary = json.load(json_file)\n",
    "            \n",
    "    #else start with empty dict\n",
    "    else:\n",
    "        property_dictionnary = {}\n",
    "\n",
    "    for record in ijson.items(f_read, \"results.bindings.item\"):\n",
    "        item = (f'<{record[prop][\"value\"]}>')\n",
    "        prop_count = property_dictionnary.get(item)\n",
    "        if prop_count: property_dictionnary[item] += 1\n",
    "        else: property_dictionnary[item] = 1\n",
    "\n",
    "    f_read.close()\n",
    "    #dump new dict\n",
    "    with open(properties_count_file, \"w\") as fp:\n",
    "        json.dump(property_dictionnary,fp) \n",
    "\n",
    "def sparql_call(sparql_query: str, result_file : str, database_name : str = \"dbpedia\") -> int :\n",
    "    \"\"\"Call a jar file to execute a sparql query on a database for a specified query \n",
    "\n",
    "    Args:\n",
    "        sparql_query (str): sparql query we want to execute on a database\n",
    "    \"\"\"\n",
    "    jar_path = \"~/../soulard/QueryHDT/SparqlToJSON.jar\"\n",
    "    database_path = {\n",
    "        \"dbpedia\" : \"~/../soulard/Graphs_HDT/DBpedia/DBpedia_en.hdt\",\n",
    "        \"wikidata\" : \"~/../soulard/Graphs_HDT/Wikidata/Wikidata_final.hdt\" \n",
    "    }\n",
    "    dataset_path = database_path[database_name]\n",
    "    #hdt_command = \"nohup java -Xmx120G -Xms120G -jar \"+jar_path+\" \"+dataset_path+\" \\\"\"+sparql_query+\"\\\" > \"+result_file+\" &\"\n",
    "    hdt_command = \"java -jar \"+jar_path+\" \"+dataset_path+\" \\\"\"+sparql_query+\"\\\" > \"+result_file\n",
    "    return os.system(hdt_command)\n",
    "\n",
    "def clean_nohup_file(result_file : str) -> None:\n",
    "    \"\"\"Commands to remove unwanted string at the start of nohup resulting files\n",
    "\n",
    "    Args:\n",
    "        result_file (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    code = 0\n",
    "    new_res_file = result_file.split(\".\")[0]+\"_clean.json\"\n",
    "    code += os.system(\"cp \"+result_file+\" \"+new_res_file)\n",
    "    code += os.system(\"awk 'NR > 3 { print }' < \"+new_res_file+\" > \"+result_file) #remove first 3 lines of the file\n",
    "    return code\n",
    "\n",
    "def concat_result_files(result_file : str, output_file : str, var_names : list) -> None:\n",
    "    \"\"\"Concat the json file to the output file\n",
    "\n",
    "    Args:\n",
    "        result_file (str): json file \n",
    "        output_file (str): ttl format file, append at the end of file\n",
    "    \"\"\"\n",
    "    \n",
    "    e,p,v = var_names\n",
    "    f_read = open(result_file, 'r', encoding=\"UTF-8\")\n",
    "    f_write = open(output_file, 'a', encoding=\"UTF-8\")\n",
    "    for record in ijson.items(f_read, \"results.bindings.item\"):\n",
    "        entity = (f'<{record[e][\"value\"]}>')\n",
    "        prop = (f'<{record[p][\"value\"]}>')\n",
    "        value = (f'{record[v][\"value\"]}')\n",
    "        f_write.write(entity+\"\\t\"+prop+\"\\t\"+value+\"\\n\")\n",
    "        \n",
    "    f_read.close()\n",
    "    f_write.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_entity_for_specific_prop(sameAs_file : str, prop : str, database_name : str) -> None:\n",
    "    \n",
    "    result_file = \"../data/result_entity_that_support_p.json\"\n",
    "    name_of_prop = prop.split(\"/\")[-1][0:-1]\n",
    "    output_file = \"../data/prop_\"+name_of_prop+\"_support_db.ttl\"\n",
    "    slice_size = 2000 #number of values in the VALUES keyword in sparql\n",
    "    db_entity_list, wk_entity_list = list(map(list,read_sameAs_file(sameAs_file))) #convert the sets back to maps\n",
    "    if database_name == \"dbpedia\":\n",
    "        sliced = property_list_cutting(db_entity_list,slice_size) #cut the big list into slice in a 2D list\n",
    "    if database_name == \"wikidata\":\n",
    "        sliced = property_list_cutting(wk_entity_list,slice_size) #cut the big list into slice in a 2D list\n",
    "    query_values_strings = [] #list of string properties used for VALUES in the sparql query\n",
    "    for i in sliced:\n",
    "        query_values_strings.append(query_generate_VALUES(i)) #join all prop in each bucket in a single string\n",
    "    \n",
    "    #call with sparql to get all properties used for each entities\n",
    "    for entity in query_values_strings:\n",
    "        sparql_query = \"\"\"select distinct ?e ?p ?v  where {\n",
    "            values ?e { \"\"\"+entity+\"\"\" }.\n",
    "            bind(\"\"\"+prop+\"\"\" as ?p)\n",
    "            ?e ?p ?v.\n",
    "            }  \n",
    "            \"\"\"\n",
    "        sparql_query = re.sub(r\"\\n|'\",\"\",sparql_query)\n",
    "        sparql_call(sparql_query, result_file, database_name)\n",
    "        concat_result_files(result_file, output_file, var_names=[\"e\",\"p\",\"v\"])\n",
    "\n",
    "def count_property_support(sameAs_file : str, database_name : str) -> None:\n",
    "    \"\"\"\n",
    "        Count the support for every property supported by entity that have a sameAs link\n",
    "    \"\"\"\n",
    "    result_file = \"../data/result_prop.json\"\n",
    "    dict_output_file = \"../data/property_support.json\"\n",
    "    slice_size = 1000 #number of values in the VALUES keyword in sparql\n",
    "\n",
    "    db_entity_list, _ = list(map(list,read_sameAs_file(sameAs_file))) #convert the sets back to maps\n",
    "    \n",
    "    sliced = property_list_cutting(db_entity_list,slice_size) #cut the big list into slice in a 2D list\n",
    "    query_values_strings = [] #list of string properties used for VALUES in the sparql query\n",
    "    for i in sliced:\n",
    "        query_values_strings.append(query_generate_VALUES(i)) #join all prop in each bucket in a single string\n",
    "    \n",
    "    #call with sparql to get all properties used for each entities\n",
    "    for entity in query_values_strings:\n",
    "        sparql_query = \"\"\"select distinct ?a ?b  where {\n",
    "            values ?a { \"\"\"+entity+\"\"\" }.\n",
    "            ?a ?b ?_.\n",
    "            }  \n",
    "            \"\"\"\n",
    "        sparql_query = re.sub(r\"\\n|'\",\"\",sparql_query)\n",
    "        \n",
    "        sparql_call(sparql_query, result_file, database_name)\n",
    "\n",
    "        read_result_file(result_file, dict_output_file, prop_var_name=\"b\")\n",
    "\n",
    "def find_entity_for_specific_prop_hdt_version(sameAs_file : str, query_service : str, prop : str, database_name : str) -> None:\n",
    "    \n",
    "    name_of_prop = prop.split(\"/\")[-1][0:-1]\n",
    "    output_file = \"../data/prop_\"+name_of_prop+\"_support_db.json\"\n",
    "    database_path = {\n",
    "    \"dbpedia\" : \"~/../soulard/Graphs_HDT/DBpedia/DBpedia_en.hdt\",\n",
    "    \"wikidata\" : \"~/../soulard/Graphs_HDT/Wikidata/Wikidata_final.hdt\" \n",
    "    }\n",
    "    db_entity_list, wk_entity_list = list(map(list,read_sameAs_file(sameAs_file))) #convert the sets back to maps\n",
    "    if database_name == \"dbpedia\":\n",
    "        entities = query_generate_VALUES(db_entity_list) #cut the big list into slice in a 2D list\n",
    "    if database_name == \"wikidata\":\n",
    "        entities = query_generate_VALUES(wk_entity_list) #cut the big list into slice in a 2D list\n",
    "\n",
    "\n",
    "    sparql_query = \"\"\"select distinct ?e ?p ?v  where {\n",
    "        values ?e { \"\"\"+entities+\"\"\" }.\n",
    "        bind(\"\"\"+prop+\"\"\" as ?p)\n",
    "        ?e ?p ?v.\n",
    "        }  \n",
    "        \"\"\"        \n",
    "    sparql_query = re.sub(r\"\\n|'\",\"\",sparql_query)\n",
    "    query_file = name_of_prop+\"_query\"\n",
    "    with open(query_file,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(sparql_query)\n",
    "\n",
    "    #call with sparql to get all entities for a specific property\n",
    "    hdt_query_command = \"java -Xmx50G -Xms50G -jar \"+query_service+\" \"+database_path+\" \"+query_file+\" \"+output_file\n",
    "    os.system(hdt_query_command)\n",
    "    print(\"##############################\")\n",
    "    print(\"#############DONE#############\")\n",
    "    print(\"#############DONE#############\")\n",
    "    print(\"#############DONE#############\")\n",
    "    print(\"##############################\")\n",
    "\n",
    "def get_sameAs_for_pop_hdt_version(ttl_file : str, query_service : str, prop : str, database_name : str) -> None:\n",
    "    \n",
    "    name_of_prop = prop.split(\"/\")[-1][0:-1]\n",
    "    output_file = \"../data/prop_\"+name_of_prop+\"_support_db.json\"\n",
    "    database_path = {\n",
    "    \"dbpedia\" : \"~/../soulard/Graphs_HDT/DBpedia/DBpedia_en.hdt\",\n",
    "    \"wikidata\" : \"~/../soulard/Graphs_HDT/Wikidata/Wikidata_final.hdt\" \n",
    "    }\n",
    "    entity,_,_ = list(map(list,read_ttl_file(ttl_file))) #convert the sets back to maps\n",
    "    entities = query_generate_VALUES(entity) #cut the big list into slice in a 2D list\n",
    "   \n",
    "    sparql_query = \"\"\"\n",
    "        PREFIX owl: <http://www.w3.org/2002/07/owl#> \n",
    "        select distinct ?e ?v  where {\n",
    "        values ?e { \"\"\"+entities+\"\"\" }. \n",
    "        ?e owl:sameAs ?v.\n",
    "        FILTER(strstarts(str(?v), 'http://www.wikidata.'))\n",
    "        }  \n",
    "        \"\"\" \n",
    "    #sparql_query = re.sub(r\"\\n|'\",\"\",sparql_query)\n",
    "    query_file = name_of_prop+\"_query\"\n",
    "    with open(query_file,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(sparql_query)\n",
    "\n",
    "    #call with sparql to get all entities for a specific property\n",
    "    hdt_query_command = \"java -Xmx50G -Xms50G -jar \"+query_service+\" \"+database_path[database_name]+\" \"+query_file+\" \"+output_file\n",
    "    os.system(hdt_query_command)\n",
    "    print(\"##############################\")\n",
    "    print(\"#############DONE#############\")\n",
    "    print(\"#############DONE#############\")\n",
    "    print(\"#############DONE#############\")\n",
    "    print(\"##############################\")\n",
    "\n",
    "def find_entity_for_specific_prop_hdt_version(sameAs_file : str, query_service : str, prop : str, database_name : str) -> None:\n",
    "    \n",
    "    name_of_prop = prop.split(\"/\")[-1][0:-1]\n",
    "    output_file = \"../data/prop_\"+name_of_prop+\"_support_db.json\"\n",
    "    database_path = {\n",
    "    \"dbpedia\" : \"~/../soulard/Graphs_HDT/DBpedia/DBpedia_en.hdt\",\n",
    "    \"wikidata\" : \"~/../soulard/Graphs_HDT/Wikidata/Wikidata_final.hdt\" \n",
    "    }\n",
    "    db_entity_list, wk_entity_list = list(map(list,read_sameAs_file(sameAs_file))) #convert the sets back to maps\n",
    "    if database_name == \"dbpedia\":\n",
    "        entities = query_generate_VALUES(db_entity_list) #cut the big list into slice in a 2D list\n",
    "    if database_name == \"wikidata\":\n",
    "        entities = query_generate_VALUES(wk_entity_list) #cut the big list into slice in a 2D list\n",
    "\n",
    "\n",
    "    sparql_query = \"\"\"select distinct ?e ?p ?v  where {\n",
    "        values ?e { \"\"\"+entities+\"\"\" }.\n",
    "        bind(\"\"\"+prop+\"\"\" as ?p)\n",
    "        ?e ?p ?v.\n",
    "        }  \n",
    "        \"\"\"        \n",
    "    sparql_query = re.sub(r\"\\n|'\",\"\",sparql_query)\n",
    "    query_file = name_of_prop+\"_query\"\n",
    "    with open(query_file,\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(sparql_query)\n",
    "\n",
    "    #call with sparql to get all entities for a specific property\n",
    "    hdt_query_command = \"java -Xmx50G -Xms50G -jar \"+query_service+\" \"+database_path+\" \"+query_file+\" \"+output_file\n",
    "    os.system(hdt_query_command)\n",
    "    print(\"##############################\")\n",
    "    print(\"#############DONE#############\")\n",
    "    print(\"#############DONE#############\")\n",
    "    print(\"#############DONE#############\")\n",
    "    print(\"##############################\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
